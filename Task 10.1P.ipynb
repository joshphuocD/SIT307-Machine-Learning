{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ebeb93b5-9832-4f84-afbe-adcc000c68da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d6b73-2f40-419c-a822-b68d08a0db44",
   "metadata": {},
   "source": [
    "* **1.Create a MLP model with 16 hidden layers using  \"fetch_lfw_people\" dataset from SKLearn and report performances using appropriate metrics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "10e27f3e-9e68-4f47-93b7-6fa41517d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "data_set = fetch_lfw_people(min_faces_per_person=60, resize=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca435a4f-9857-4925-b03f-3c6e428a34f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2904</th>\n",
       "      <th>2905</th>\n",
       "      <th>2906</th>\n",
       "      <th>2907</th>\n",
       "      <th>2908</th>\n",
       "      <th>2909</th>\n",
       "      <th>2910</th>\n",
       "      <th>2911</th>\n",
       "      <th>2912</th>\n",
       "      <th>2913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.534641</td>\n",
       "      <td>0.525490</td>\n",
       "      <td>0.496732</td>\n",
       "      <td>0.467974</td>\n",
       "      <td>0.452288</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>0.486275</td>\n",
       "      <td>0.516340</td>\n",
       "      <td>0.532026</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464052</td>\n",
       "      <td>0.071895</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.010458</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.002614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.286275</td>\n",
       "      <td>0.207843</td>\n",
       "      <td>0.252288</td>\n",
       "      <td>0.362092</td>\n",
       "      <td>0.366013</td>\n",
       "      <td>0.260131</td>\n",
       "      <td>0.394771</td>\n",
       "      <td>0.498039</td>\n",
       "      <td>0.484967</td>\n",
       "      <td>0.496732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963399</td>\n",
       "      <td>0.981699</td>\n",
       "      <td>0.993464</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.990850</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>0.969935</td>\n",
       "      <td>0.949020</td>\n",
       "      <td>0.934641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.318954</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.275817</td>\n",
       "      <td>0.193464</td>\n",
       "      <td>0.368627</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.478431</td>\n",
       "      <td>0.498039</td>\n",
       "      <td>0.504575</td>\n",
       "      <td>0.526797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322876</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.420915</td>\n",
       "      <td>0.453595</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.464052</td>\n",
       "      <td>0.381699</td>\n",
       "      <td>0.426144</td>\n",
       "      <td>0.790850</td>\n",
       "      <td>0.955555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.484967</td>\n",
       "      <td>0.532026</td>\n",
       "      <td>0.543791</td>\n",
       "      <td>0.552941</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.579085</td>\n",
       "      <td>0.626144</td>\n",
       "      <td>0.677124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.390850</td>\n",
       "      <td>0.665359</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.896732</td>\n",
       "      <td>0.900654</td>\n",
       "      <td>0.903268</td>\n",
       "      <td>0.886275</td>\n",
       "      <td>0.802614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.220915</td>\n",
       "      <td>0.186928</td>\n",
       "      <td>0.183007</td>\n",
       "      <td>0.202614</td>\n",
       "      <td>0.173856</td>\n",
       "      <td>0.172549</td>\n",
       "      <td>0.201307</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.214379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.146405</td>\n",
       "      <td>0.134641</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.132026</td>\n",
       "      <td>0.135948</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.218301</td>\n",
       "      <td>0.207843</td>\n",
       "      <td>0.250980</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.433987</td>\n",
       "      <td>0.551634</td>\n",
       "      <td>0.624837</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326797</td>\n",
       "      <td>0.383007</td>\n",
       "      <td>0.359477</td>\n",
       "      <td>0.367320</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.367320</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.369935</td>\n",
       "      <td>0.450980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>0.359477</td>\n",
       "      <td>0.349020</td>\n",
       "      <td>0.320261</td>\n",
       "      <td>0.308497</td>\n",
       "      <td>0.354248</td>\n",
       "      <td>0.456209</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.662745</td>\n",
       "      <td>0.715033</td>\n",
       "      <td>0.724183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232680</td>\n",
       "      <td>0.227451</td>\n",
       "      <td>0.207843</td>\n",
       "      <td>0.189542</td>\n",
       "      <td>0.183007</td>\n",
       "      <td>0.186928</td>\n",
       "      <td>0.198693</td>\n",
       "      <td>0.216993</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.177778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>0.116340</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.101961</td>\n",
       "      <td>0.169935</td>\n",
       "      <td>0.363399</td>\n",
       "      <td>0.383007</td>\n",
       "      <td>0.334641</td>\n",
       "      <td>0.277124</td>\n",
       "      <td>0.473203</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525490</td>\n",
       "      <td>0.524183</td>\n",
       "      <td>0.518954</td>\n",
       "      <td>0.526797</td>\n",
       "      <td>0.556863</td>\n",
       "      <td>0.584314</td>\n",
       "      <td>0.569935</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.580392</td>\n",
       "      <td>0.554248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>0.193464</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.290196</td>\n",
       "      <td>0.351634</td>\n",
       "      <td>0.292810</td>\n",
       "      <td>0.279739</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.456209</td>\n",
       "      <td>0.426144</td>\n",
       "      <td>0.292810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590850</td>\n",
       "      <td>0.641830</td>\n",
       "      <td>0.632680</td>\n",
       "      <td>0.641830</td>\n",
       "      <td>0.687582</td>\n",
       "      <td>0.762091</td>\n",
       "      <td>0.747712</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.654902</td>\n",
       "      <td>0.590850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>0.122876</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.109804</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.162092</td>\n",
       "      <td>0.241830</td>\n",
       "      <td>0.350327</td>\n",
       "      <td>0.449673</td>\n",
       "      <td>0.499346</td>\n",
       "      <td>0.522876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>0.257516</td>\n",
       "      <td>0.243137</td>\n",
       "      <td>0.499346</td>\n",
       "      <td>0.492810</td>\n",
       "      <td>0.227451</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.163399</td>\n",
       "      <td>0.291503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1348 rows × 2914 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     0.534641  0.525490  0.496732  0.467974  0.452288  0.462745  0.486275   \n",
       "1     0.286275  0.207843  0.252288  0.362092  0.366013  0.260131  0.394771   \n",
       "2     0.318954  0.392157  0.275817  0.193464  0.368627  0.466667  0.478431   \n",
       "3     0.258824  0.305882  0.484967  0.532026  0.543791  0.552941  0.555556   \n",
       "4     0.222222  0.220915  0.186928  0.183007  0.202614  0.173856  0.172549   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1343  0.211765  0.218301  0.207843  0.250980  0.325490  0.433987  0.551634   \n",
       "1344  0.359477  0.349020  0.320261  0.308497  0.354248  0.456209  0.576471   \n",
       "1345  0.116340  0.111111  0.101961  0.169935  0.363399  0.383007  0.334641   \n",
       "1346  0.193464  0.211765  0.290196  0.351634  0.292810  0.279739  0.376471   \n",
       "1347  0.122876  0.098039  0.109804  0.129412  0.162092  0.241830  0.350327   \n",
       "\n",
       "          7         8         9     ...      2904      2905      2906  \\\n",
       "0     0.516340  0.532026  0.555556  ...  0.464052  0.071895  0.006536   \n",
       "1     0.498039  0.484967  0.496732  ...  0.963399  0.981699  0.993464   \n",
       "2     0.498039  0.504575  0.526797  ...  0.322876  0.352941  0.420915   \n",
       "3     0.579085  0.626144  0.677124  ...  0.345098  0.390850  0.665359   \n",
       "4     0.201307  0.247059  0.214379  ...  0.164706  0.156863  0.146405   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1343  0.624837  0.644444  0.658824  ...  0.326797  0.383007  0.359477   \n",
       "1344  0.662745  0.715033  0.724183  ...  0.232680  0.227451  0.207843   \n",
       "1345  0.277124  0.473203  0.623529  ...  0.525490  0.524183  0.518954   \n",
       "1346  0.456209  0.426144  0.292810  ...  0.590850  0.641830  0.632680   \n",
       "1347  0.449673  0.499346  0.522876  ...  0.262745  0.257516  0.243137   \n",
       "\n",
       "          2907      2908      2909      2910      2911      2912      2913  \n",
       "0     0.010458  0.005229  0.000000  0.003922  0.006536  0.006536  0.002614  \n",
       "1     0.996078  0.990850  0.996078  0.988235  0.969935  0.949020  0.934641  \n",
       "2     0.453595  0.482353  0.464052  0.381699  0.426144  0.790850  0.955555  \n",
       "3     0.915033  0.917647  0.896732  0.900654  0.903268  0.886275  0.802614  \n",
       "4     0.134641  0.133333  0.137255  0.132026  0.135948  0.141176  0.133333  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1343  0.367320  0.377778  0.367320  0.352941  0.352941  0.369935  0.450980  \n",
       "1344  0.189542  0.183007  0.186928  0.198693  0.216993  0.215686  0.177778  \n",
       "1345  0.526797  0.556863  0.584314  0.569935  0.568627  0.580392  0.554248  \n",
       "1346  0.641830  0.687582  0.762091  0.747712  0.686275  0.654902  0.590850  \n",
       "1347  0.499346  0.492810  0.227451  0.156863  0.129412  0.163399  0.291503  \n",
       "\n",
       "[1348 rows x 2914 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get features\n",
    "X = pd.DataFrame(data_set.data).reset_index(drop=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "efcac124-922c-4dfc-b3fc-3b8db4095233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2904</th>\n",
       "      <th>2905</th>\n",
       "      <th>2906</th>\n",
       "      <th>2907</th>\n",
       "      <th>2908</th>\n",
       "      <th>2909</th>\n",
       "      <th>2910</th>\n",
       "      <th>2911</th>\n",
       "      <th>2912</th>\n",
       "      <th>2913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "      <td>1348.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.349990</td>\n",
       "      <td>0.360421</td>\n",
       "      <td>0.381791</td>\n",
       "      <td>0.415259</td>\n",
       "      <td>0.455019</td>\n",
       "      <td>0.491307</td>\n",
       "      <td>0.521955</td>\n",
       "      <td>0.546534</td>\n",
       "      <td>0.565645</td>\n",
       "      <td>0.581154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421755</td>\n",
       "      <td>0.449288</td>\n",
       "      <td>0.476244</td>\n",
       "      <td>0.495646</td>\n",
       "      <td>0.500977</td>\n",
       "      <td>0.490644</td>\n",
       "      <td>0.474444</td>\n",
       "      <td>0.455665</td>\n",
       "      <td>0.430128</td>\n",
       "      <td>0.405530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.179821</td>\n",
       "      <td>0.176789</td>\n",
       "      <td>0.173750</td>\n",
       "      <td>0.171163</td>\n",
       "      <td>0.167618</td>\n",
       "      <td>0.163853</td>\n",
       "      <td>0.155031</td>\n",
       "      <td>0.146829</td>\n",
       "      <td>0.142565</td>\n",
       "      <td>0.140048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231421</td>\n",
       "      <td>0.248237</td>\n",
       "      <td>0.266033</td>\n",
       "      <td>0.279998</td>\n",
       "      <td>0.288290</td>\n",
       "      <td>0.296076</td>\n",
       "      <td>0.302288</td>\n",
       "      <td>0.305198</td>\n",
       "      <td>0.304334</td>\n",
       "      <td>0.305346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.024837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.218301</td>\n",
       "      <td>0.233660</td>\n",
       "      <td>0.260131</td>\n",
       "      <td>0.299020</td>\n",
       "      <td>0.350327</td>\n",
       "      <td>0.390850</td>\n",
       "      <td>0.427451</td>\n",
       "      <td>0.453595</td>\n",
       "      <td>0.477124</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248039</td>\n",
       "      <td>0.261111</td>\n",
       "      <td>0.269281</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>0.250980</td>\n",
       "      <td>0.223203</td>\n",
       "      <td>0.186601</td>\n",
       "      <td>0.169608</td>\n",
       "      <td>0.149020</td>\n",
       "      <td>0.128105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.332026</td>\n",
       "      <td>0.349020</td>\n",
       "      <td>0.377124</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.454902</td>\n",
       "      <td>0.492810</td>\n",
       "      <td>0.526144</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.570588</td>\n",
       "      <td>0.586928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>0.420915</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.473203</td>\n",
       "      <td>0.483660</td>\n",
       "      <td>0.475817</td>\n",
       "      <td>0.458170</td>\n",
       "      <td>0.421569</td>\n",
       "      <td>0.370588</td>\n",
       "      <td>0.325490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.469281</td>\n",
       "      <td>0.474510</td>\n",
       "      <td>0.493137</td>\n",
       "      <td>0.528431</td>\n",
       "      <td>0.567320</td>\n",
       "      <td>0.601307</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.643137</td>\n",
       "      <td>0.656209</td>\n",
       "      <td>0.674510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.562418</td>\n",
       "      <td>0.621242</td>\n",
       "      <td>0.678758</td>\n",
       "      <td>0.727124</td>\n",
       "      <td>0.749020</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.741503</td>\n",
       "      <td>0.726797</td>\n",
       "      <td>0.695752</td>\n",
       "      <td>0.669608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.997386</td>\n",
       "      <td>0.997386</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.986928</td>\n",
       "      <td>0.969935</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.959477</td>\n",
       "      <td>0.975163</td>\n",
       "      <td>0.979085</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993464</td>\n",
       "      <td>0.997386</td>\n",
       "      <td>0.998693</td>\n",
       "      <td>0.998693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998693</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 2914 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0            1            2            3            4     \\\n",
       "count  1348.000000  1348.000000  1348.000000  1348.000000  1348.000000   \n",
       "mean      0.349990     0.360421     0.381791     0.415259     0.455019   \n",
       "std       0.179821     0.176789     0.173750     0.171163     0.167618   \n",
       "min       0.000000     0.001307     0.000000     0.002614     0.002614   \n",
       "25%       0.218301     0.233660     0.260131     0.299020     0.350327   \n",
       "50%       0.332026     0.349020     0.377124     0.411765     0.454902   \n",
       "75%       0.469281     0.474510     0.493137     0.528431     0.567320   \n",
       "max       0.997386     0.997386     0.996078     0.986928     0.969935   \n",
       "\n",
       "              5            6            7            8            9     ...  \\\n",
       "count  1348.000000  1348.000000  1348.000000  1348.000000  1348.000000  ...   \n",
       "mean      0.491307     0.521955     0.546534     0.565645     0.581154  ...   \n",
       "std       0.163853     0.155031     0.146829     0.142565     0.140048  ...   \n",
       "min       0.005229     0.007843     0.005229     0.011765     0.024837  ...   \n",
       "25%       0.390850     0.427451     0.453595     0.477124     0.494118  ...   \n",
       "50%       0.492810     0.526144     0.549020     0.570588     0.586928  ...   \n",
       "75%       0.601307     0.623529     0.643137     0.656209     0.674510  ...   \n",
       "max       0.960784     0.959477     0.975163     0.979085     0.992157  ...   \n",
       "\n",
       "              2904         2905         2906         2907         2908  \\\n",
       "count  1348.000000  1348.000000  1348.000000  1348.000000  1348.000000   \n",
       "mean      0.421755     0.449288     0.476244     0.495646     0.500977   \n",
       "std       0.231421     0.248237     0.266033     0.279998     0.288290   \n",
       "min       0.003922     0.003922     0.003922     0.002614     0.001307   \n",
       "25%       0.248039     0.261111     0.269281     0.262745     0.250980   \n",
       "50%       0.398693     0.420915     0.450980     0.473203     0.483660   \n",
       "75%       0.562418     0.621242     0.678758     0.727124     0.749020   \n",
       "max       0.993464     0.997386     0.998693     0.998693     1.000000   \n",
       "\n",
       "              2909         2910         2911         2912         2913  \n",
       "count  1348.000000  1348.000000  1348.000000  1348.000000  1348.000000  \n",
       "mean      0.490644     0.474444     0.455665     0.430128     0.405530  \n",
       "std       0.296076     0.302288     0.305198     0.304334     0.305346  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.223203     0.186601     0.169608     0.149020     0.128105  \n",
       "50%       0.475817     0.458170     0.421569     0.370588     0.325490  \n",
       "75%       0.752941     0.741503     0.726797     0.695752     0.669608  \n",
       "max       1.000000     1.000000     1.000000     0.998693     1.000000  \n",
       "\n",
       "[8 rows x 2914 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4454ca59-d8b6-40ec-a564-bfeea9266002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1348 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0     1\n",
       "1     3\n",
       "2     3\n",
       "3     3\n",
       "4     5\n",
       "...  ..\n",
       "1343  5\n",
       "1344  5\n",
       "1345  7\n",
       "1346  3\n",
       "1347  5\n",
       "\n",
       "[1348 rows x 1 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get target\n",
    "y = pd.DataFrame(data_set.target)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7a49c38a-1758-47b3-8466-0ac36a64581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize dataset\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "df444a26-ac21-45c1-b3ca-06bce3c16818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" checked><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e303b638-c212-476d-8911-ebb4e1154845",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c7715751-7011-4b7b-8d06-f74e3e0d0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "43930d7d-0ca5-4e00-bd54-8ea9c43d60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build multilayer perceptron model with 16 hidden layers\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(16,)*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "94028d31-cbd3-4cfa-84b7-41dc9822c42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "                                  16, 16, 16, 16, 16))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "                                  16, 16, 16, 16, 16))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "                                  16, 16, 16, 16, 16))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "79471ad6-4cc2-4144-86bd-381ecf3c2d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.6222222222222222\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.59      0.53        17\n",
      "           1       0.73      0.73      0.73        84\n",
      "           2       0.65      0.42      0.51        36\n",
      "           3       0.79      0.73      0.76       146\n",
      "           4       0.27      0.61      0.37        28\n",
      "           5       0.53      0.33      0.41        27\n",
      "           6       0.44      0.50      0.47        16\n",
      "           7       0.57      0.49      0.53        51\n",
      "\n",
      "    accuracy                           0.62       405\n",
      "   macro avg       0.56      0.55      0.54       405\n",
      "weighted avg       0.66      0.62      0.63       405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict = classifier.predict(X_test)\n",
    "print('Accuracy score: ', accuracy_score(y_test, y_predict))\n",
    "print('Classification report: \\n', classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39c30a-e010-4a1a-ae32-20da234d3b54",
   "metadata": {},
   "source": [
    " * **2.Analyse impact of different activation function and solver on the model.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d629e-c6e8-4689-85fa-407f2a183954",
   "metadata": {},
   "source": [
    "**The default activation function is Rectified Linear Unit (reLU), let's try different activation functions such as hyperbolic tangent activation function (tanh) or logistic activation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a27641a0-d26b-4c1f-8046-9f2438b4340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tanh activation function\n",
    "tanh_classifier = MLPClassifier(hidden_layer_sizes=(16,)*16, activation='tanh',max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cce9f924-32bc-48ed-bc4c-e6ed9ae995fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.6765432098765433\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.53      0.53        17\n",
      "           1       0.69      0.79      0.74        84\n",
      "           2       0.61      0.64      0.62        36\n",
      "           3       0.84      0.75      0.79       146\n",
      "           4       0.52      0.54      0.53        28\n",
      "           5       0.27      0.30      0.28        27\n",
      "           6       0.48      0.69      0.56        16\n",
      "           7       0.76      0.63      0.69        51\n",
      "\n",
      "    accuracy                           0.68       405\n",
      "   macro avg       0.59      0.61      0.59       405\n",
      "weighted avg       0.69      0.68      0.68       405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tanh_classifier.fit(X_train, y_train.values.ravel())\n",
    "print('Accuracy score: ', accuracy_score(y_test, tanh_classifier.predict(X_test)))\n",
    "print('Classification report: \\n', classification_report(y_test, tanh_classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "89526acf-a010-49db-8e65-a58cbcbd676f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.36049382716049383\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        17\n",
      "           1       0.00      0.00      0.00        84\n",
      "           2       0.00      0.00      0.00        36\n",
      "           3       0.36      1.00      0.53       146\n",
      "           4       0.00      0.00      0.00        28\n",
      "           5       0.00      0.00      0.00        27\n",
      "           6       0.00      0.00      0.00        16\n",
      "           7       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.36       405\n",
      "   macro avg       0.05      0.12      0.07       405\n",
      "weighted avg       0.13      0.36      0.19       405\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "logistics_classifier = MLPClassifier(hidden_layer_sizes=(16,)*16, activation='logistic', max_iter=2000)\n",
    "logistics_classifier.fit(X_train, y_train.values.ravel())\n",
    "print('Accuracy score: ', accuracy_score(y_test, logistics_classifier.predict(X_test)))\n",
    "print('Classification report: \\n', classification_report(y_test, logistics_classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a81cc9-8bf0-431f-80aa-47563866b81d",
   "metadata": {},
   "source": [
    "**Let's use Grid Search Cross Validation to find the best parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5bd04a43-c29c-402d-8904-dbea9f083177",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "        {\n",
    "            'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            'solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "            'max_iter': [2000]\n",
    "        }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "63d4d518-d626-41f1-9bcf-c3d5cce3077e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-15 {color: black;background-color: white;}#sk-container-id-15 pre{padding: 0;}#sk-container-id-15 div.sk-toggleable {background-color: white;}#sk-container-id-15 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-15 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-15 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-15 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-15 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-15 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-15 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-15 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-15 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-15 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-15 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-15 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-15 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-15 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-15 div.sk-item {position: relative;z-index: 1;}#sk-container-id-15 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-15 div.sk-item::before, #sk-container-id-15 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-15 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-15 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-15 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-15 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-15 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-15 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-15 div.sk-label-container {text-align: center;}#sk-container-id-15 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-15 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-15\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=MLPClassifier(),\n",
       "             param_grid=[{&#x27;activation&#x27;: [&#x27;identity&#x27;, &#x27;logistic&#x27;, &#x27;tanh&#x27;,\n",
       "                                         &#x27;relu&#x27;],\n",
       "                          &#x27;max_iter&#x27;: [2000],\n",
       "                          &#x27;solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;sgd&#x27;, &#x27;adam&#x27;]}],\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=MLPClassifier(),\n",
       "             param_grid=[{&#x27;activation&#x27;: [&#x27;identity&#x27;, &#x27;logistic&#x27;, &#x27;tanh&#x27;,\n",
       "                                         &#x27;relu&#x27;],\n",
       "                          &#x27;max_iter&#x27;: [2000],\n",
       "                          &#x27;solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;sgd&#x27;, &#x27;adam&#x27;]}],\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=MLPClassifier(),\n",
       "             param_grid=[{'activation': ['identity', 'logistic', 'tanh',\n",
       "                                         'relu'],\n",
       "                          'max_iter': [2000],\n",
       "                          'solver': ['lbfgs', 'sgd', 'adam']}],\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(MLPClassifier(), param_grid, cv=3,\n",
    "                           scoring='accuracy')\n",
    "clf.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c41eab26-88be-4c79-b20a-377469ad809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'activation': 'logistic', 'max_iter': 2000, 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b9605dcc-2783-4a9f-8cd4-02b7e2a1a8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.8320987654320988\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.82      0.78        17\n",
      "           1       0.82      0.85      0.83        84\n",
      "           2       0.81      0.81      0.81        36\n",
      "           3       0.89      0.83      0.86       146\n",
      "           4       0.75      0.86      0.80        28\n",
      "           5       0.83      0.70      0.76        27\n",
      "           6       0.84      1.00      0.91        16\n",
      "           7       0.81      0.84      0.83        51\n",
      "\n",
      "    accuracy                           0.83       405\n",
      "   macro avg       0.81      0.84      0.82       405\n",
      "weighted avg       0.83      0.83      0.83       405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_classifier = MLPClassifier(activation='logistic', max_iter=2000, solver = 'adam')\n",
    "best_classifier.fit(X_train, y_train.values.ravel())\n",
    "print('Accuracy score: ', accuracy_score(y_test, best_classifier.predict(X_test)))\n",
    "print('Classification report: \\n', classification_report(y_test, best_classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d84bff-bdd6-4df9-864c-8b643b1d32f7",
   "metadata": {},
   "source": [
    "* **3.Explain your findings and report the best performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fc8798-0db0-4292-86ac-36f2a038709b",
   "metadata": {},
   "source": [
    "The accuary scores have been improved by using different activation functions. With the default activation function, the accuracy socre is only 76.5%, but then changed to 78.5 using tanh activation function and 80% using logistics functions. The accuracy scores for each target are also changing. And by using grid search cross validation, we can find the best parameters for the model which is logistic for activation function and adam for solver."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
